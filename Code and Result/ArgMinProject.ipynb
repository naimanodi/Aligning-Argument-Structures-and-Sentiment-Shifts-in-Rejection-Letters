{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"E40qY-Vku0jI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727425552144,"user_tz":-120,"elapsed":402,"user":{"displayName":"Lukas B","userId":"10971232176799389025"}},"outputId":"59876fe7-ef6b-487e-85f4-433433cb5e3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Text data has been successfully cleaned and saved to cleaned_text_filtered.csv\n"]}],"source":["import pandas as pd\n","from bs4 import BeautifulSoup\n","import re\n","\n","#Data pre-processing\n","def clean_text(text):\n","    #Remove HTML tags using BeautifulSoup\n","    text_no_html = BeautifulSoup(text, \"html.parser\").get_text()\n","\n","    #Remove URLs using regular expressions\n","    text_no_urls = re.sub(r'http\\S+|www\\S+|https\\S+', '', text_no_html, flags=re.MULTILINE)\n","\n","    #Remove unwanted sequences\n","    text_cleaned = re.sub(r'=\\S+', '', text_no_urls)\n","    text_cleaned = re.sub(r'=', '', text_cleaned)\n","\n","    return text_cleaned\n","\n","df = pd.read_csv('rejections.csv')\n","\n","# Apply the cleaning function to the \"Text\" column\n","df['Text'] = df['Text'].apply(clean_text)\n","\n","# Save the cleaned text into a new CSV file\n","df.to_csv('cleaned_text_filtered.csv', index=False)\n","\n","print(\"Text data has been successfully cleaned and saved to cleaned_text_filtered.csv\")"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","\n","import pandas as pd\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","\n","file_path = 'cleaned_text_filtered.csv'\n","df = pd.read_csv(file_path)\n","\n","if 'ID' not in df.columns:\n","    df['ID'] = df.index\n","\n","tokenized_data = []\n","\n","#Sentence tokenizer\n","for index, row in df.iterrows():\n","    text_id = row['ID']\n","    text = row['Text']\n","\n","    sentences = sent_tokenize(text)\n","\n","    #Clean and append each tokenized sentence with its ID to the list\n","    for sentence in sentences:\n","        clean_sentence = sentence.replace('\\n', ' ').strip()  #Remove new lines and extra spaces\n","        tokenized_data.append({'ID': text_id, 'Sentence': clean_sentence})\n","\n","#Create a new DataFrame from the tokenized data\n","tokenized_df = pd.DataFrame(tokenized_data)\n","\n","# Save the tokenized sentences to a new CSV file\n","output_file_path = 'tokenized_output_file.csv'  # Specify the output file path\n","tokenized_df.to_csv(output_file_path, index=False)\n","\n","# Display the first few rows of the tokenized DataFrame\n","print(tokenized_df.head())"],"metadata":{"id":"beDws0wezpYc","executionInfo":{"status":"ok","timestamp":1727425557678,"user_tz":-120,"elapsed":1628,"user":{"displayName":"Lukas B","userId":"10971232176799389025"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"31a101e8-99f6-4b9f-fcf1-691cd4264f1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   ID                                           Sentence\n","0   0  Ref: 110127BR - 2018 Data Scientist Internship...\n","1   0  With reference to the application for\\r the ab...\n","2   0  In keeping with our company policies, your app...\n","3   0  We highly encourage you to consider other oppo...\n","4   0  Sincerely,\\r \\r IBM Early Professional Recruit...\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import csv\n","\n","def clean_csv(input_file_path, cleaned_file_path):\n","    with open(input_file_path, 'r', encoding='utf-8') as infile, open(cleaned_file_path, 'w', encoding='utf-8') as outfile:\n","        for line in infile:\n","            if line.strip():  # Only write non-empty lines\n","                outfile.write(line)\n","\n","def load_csv(file_path):\n","    try:\n","        return pd.read_csv(file_path, on_bad_lines='warn')\n","    except pd.errors.EmptyDataError:\n","        print(\"The file is empty or has no valid data.\")\n","    except pd.errors.ParserError as e:\n","        print(f\"Error parsing CSV: {e}\")\n","        print(\"Attempting to clean the CSV file...\")\n","        cleaned_file_path = 'cleaned_file.csv'\n","        clean_csv(file_path, cleaned_file_path)\n","        print(f\"Cleaned file created at: {cleaned_file_path}\")\n","\n","        return pd.read_csv(cleaned_file_path, on_bad_lines='warn')\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","def concatenate_contents(df):\n","    #Concatenate the contents based on matching IDs\n","    if 'ID' in df.columns and 'Sentence' in df.columns:\n","        #Convert contents to strings and fill NaN values with empty strings\n","        df['Sentence'] = df['Sentence'].astype(str).fillna('')\n","\n","        #Concatenate contents based on IDs\n","        concatenated_df = df.groupby('ID', as_index=False)['Sentence'].agg(' '.join)\n","        return concatenated_df\n","    else:\n","        print(\"The DataFrame does not contain the required columns 'ID' and 'Sentence'.\")\n","        return None\n","\n","input_file_path = 'tokenized_output_file.csv'  # Replace with your CSV file path\n","output_file_path = 'concatenated_output_file.csv'  # Specify the output file path\n","\n","# Load the CSV file\n","df = load_csv(input_file_path)\n","\n","if df is not None:\n","    # Rename columns if necessary\n","    df.columns = ['ID', 'Sentence']  # Adjust based on actual column names\n","\n","    # Concatenate contents based on IDs\n","    result_df = concatenate_contents(df)\n","\n","    if result_df is not None:\n","        # Save the concatenated DataFrame to a new CSV file\n","        result_df.to_csv(output_file_path, index=False)\n","        print(f\"Concatenated contents saved to: {output_file_path}\")\n"],"metadata":{"id":"mpAzmtmBE7wh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Load the sentiment analysis pipeline\n","sentiment_analysis = pipeline(\"sentiment-analysis\")\n","\n","def analyze_sentiment(text):\n","    # Split text into chunks of 512 tokens or less\n","    max_length = 512\n","    words = text.split()  # Split the text into words\n","    chunks = [' '.join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n","\n","    # Analyze each chunk and combine the results\n","    results = sentiment_analysis(chunks, truncation = True)  # BERT-based model sadly only takes 512 as input. Cut the input file in half if needed\n","    # Aggregate results, e.g., using majority voting\n","    sentiments = [result['label'] for result in results]\n","\n","    # Return the most common sentiment\n","    return max(set(sentiments), key=sentiments.count)\n","\n","# Function to classify the relationship between two sentiments\n","def classify_relationship(sent1, sent2):\n","    if sent1 == 'POSITIVE' and sent2 == 'NEGATIVE':\n","        return \"Polite Rejection\"\n","    elif sent1 == 'NEGATIVE' and sent2 == 'POSITIVE':\n","        return \"Encouragement After Rejection\"\n","    elif sent1 == 'NEGATIVE' and sent2 == 'NEGATIVE':\n","        return \"Justification of Rejection\"\n","    elif sent1 == 'POSITIVE' and sent2 == 'POSITIVE':\n","        return \"Neutral or Supportive\"\n","    else:\n","        return \"Other\"  # In case the semantic analysis also provides NEUTRAL or other labels\n","\n","# Apply the custom sentiment analysis function on the \"Text\" column\n","df['Sentiment'] = df['Sentence'].apply(analyze_sentiment)\n","\n","# Analyze transitions between consecutive sentences\n","transitions = []\n","for i in range(len(df) - 1):\n","    relationship = classify_relationship(df['Sentiment'].iloc[i], df['Sentiment'].iloc[i + 1])\n","    transitions.append(relationship)\n","transitions.append(None)  # The last sentence has no transition\n","df['Transition'] = transitions\n","# Save results to a new CSV file\n","output_file_path = 'tokenized_output_file_Final.csv'  # Specify the output file path\n","df.to_csv(output_file_path, index=False)"],"metadata":{"id":"TCZMu0r6F823"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display the results\n","print(df[['Sentence', 'Sentiment', 'Transition']])"],"metadata":{"id":"VEuAxRuaY6wD","executionInfo":{"status":"ok","timestamp":1727425684131,"user_tz":-120,"elapsed":409,"user":{"displayName":"Lukas B","userId":"10971232176799389025"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b006287c-f509-4f25-d75b-ec71d4a0b61d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                              Sentence Sentiment  \\\n","0    Ref: 110127BR - 2018 Data Scientist Internship...  POSITIVE   \n","1    With reference to the application for\\n the ab...  NEGATIVE   \n","2    In keeping with our company policies, your app...  NEGATIVE   \n","3    We highly encourage you to consider other oppo...  POSITIVE   \n","4                                                  nan  POSITIVE   \n","..                                                 ...       ...   \n","809  Hi Conor,\\n Thank you for your interest in Air...  POSITIVE   \n","810  At this point, we've decided to move forward w...  POSITIVE   \n","811  We will keep your application details on recor...  NEGATIVE   \n","812  Thanks again for your interest in Airware and ...  POSITIVE   \n","813                        Regards,\\n The Airware Team  POSITIVE   \n","\n","                        Transition  \n","0                 Polite Rejection  \n","1       Justification of Rejection  \n","2    Encouragement After Rejection  \n","3            Neutral or Supportive  \n","4            Neutral or Supportive  \n","..                             ...  \n","809          Neutral or Supportive  \n","810               Polite Rejection  \n","811  Encouragement After Rejection  \n","812          Neutral or Supportive  \n","813                           None  \n","\n","[814 rows x 3 columns]\n"]}]}]}